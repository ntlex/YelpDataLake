{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "07fc6670",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:'PYARROW_IGNORE_TIMEZONE' environment variable was not set. It is required to set this environment variable to '1' in both driver and executor sides if you use pyarrow>=2.0.0. pandas-on-Spark will set it for you but it does not work if there is a Spark context already launched.\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import pyspark.pandas as ps\n",
    "\n",
    "import pyspark.sql.functions as Function\n",
    "from pyspark.sql.functions import when\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.functions import split, explode, weekofyear, year, avg, col, count\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a522e5af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/08 14:44:14 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "            .appName('yelp_processor') \\\n",
    "            .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "337e256f",
   "metadata": {},
   "source": [
    "#### First let's have a look into the `yelp_academic_dataset_business.json`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "51b8f06a",
   "metadata": {},
   "outputs": [],
   "source": [
    "business_data = f'./yelp_dataset/raw_data/yelp_academic_dataset_business.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fbf7a3a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df = spark.read.json(business_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "74149b09",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[summary: string, address: string, business_id: string, categories: string, city: string, is_open: string, latitude: string, longitude: string, name: string, postal_code: string, review_count: string, stars: string, state: string]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9445cadc",
   "metadata": {},
   "outputs": [],
   "source": [
    "pddf = df.pandas_api()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "583f79b1",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Series' object has no attribute 'count_values'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [37], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mpddf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mstate\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcount_values\u001b[49m()\n",
      "File \u001b[0;32m~/miniforge3/envs/Yelp_Data_Engineering/lib/python3.10/site-packages/pyspark/pandas/series.py:6436\u001b[0m, in \u001b[0;36mSeries.__getattr__\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m   6434\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   6435\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m partial(property_or_func, \u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m-> 6436\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSeries\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(item))\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Series' object has no attribute 'count_values'"
     ]
    }
   ],
   "source": [
    "pddf['state'].count_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93a9e562",
   "metadata": {},
   "source": [
    "#### Let's cherry pick some data from the state of AZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8794aa98",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_businesses = pddf[pddf['state'] == 'IL']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c2a8428b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2145"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(filtered_businesses.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b60a79b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_businesses = filtered_businesses[filtered_businesses['is_open'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c3f008ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8108"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(filtered_businesses.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fc4a5ff1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>address</th>\n",
       "      <th>attributes</th>\n",
       "      <th>business_id</th>\n",
       "      <th>categories</th>\n",
       "      <th>city</th>\n",
       "      <th>hours</th>\n",
       "      <th>is_open</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>name</th>\n",
       "      <th>postal_code</th>\n",
       "      <th>review_count</th>\n",
       "      <th>stars</th>\n",
       "      <th>state</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [address, attributes, business_id, categories, city, hours, is_open, latitude, longitude, name, postal_code, review_count, stars, state]\n",
       "Index: []"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_businesses.drop('any')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a302c7b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8108"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(filtered_businesses.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5183f205",
   "metadata": {},
   "source": [
    "#### Now that I have cherry picked my businesses, I will try to get the reviews that correspond to the cherrypicked business ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "af1bd40e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "review_path = './yelp_dataset/raw_data/yelp_academic_dataset_review.json'\n",
    "review_df = spark.read.json(review_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f1b98a2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[summary: string, business_id: string, cool: string, date: string, funny: string, review_id: string, stars: string, text: string, useful: string, user_id: string]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "86ad8706",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(review_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d0c3fd",
   "metadata": {},
   "source": [
    "Intially I was exploring the data as a pandas dataframe, but soon enought I realised that treating them with sql queries was more flexible. And this is where the adventure begins!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7a321406",
   "metadata": {},
   "outputs": [],
   "source": [
    "review_df.createOrReplaceTempView(\"reviews\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c921b63e",
   "metadata": {},
   "outputs": [],
   "source": [
    "rsqlDF = spark.sql('select * from reviews')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "646f4f5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "6990280"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rsqlDF.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bb821a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = rsqlDF.join(sqlDF,'business_id','leftsemi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cd24f5e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "986088"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3.count()\n",
    "\n",
    "# write to json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f846f069",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[summary: string, business_id: string, cool: string, date: string, funny: string, review_id: string, stars: string, text: string, useful: string, user_id: string]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "173b6c79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+----+-------------------+-----+----------------------+-----+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------+----------------------+\n",
      "|business_id           |cool|date               |funny|review_id             |stars|text                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   |useful|user_id               |\n",
      "+----------------------+----+-------------------+-----+----------------------+-----+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------+----------------------+\n",
      "|uMvVYRgGNXf5boolA9HXTw|0   |2015-06-21 14:48:06|0    |rGQRf8UafX7OTlMNN19I8A|5.0  |My experience with Shalimar was nothing but wonderful. \\nI wanted to get my engagement ring sized and was told over the phone that it could probably be done within the day. \\nWhen I brought it by, the team confirmed that the jeweler would be able to accommodate my same-day request and that it would be around $40 (simple band, decrease by three full sizes).\\nI checked my size one more time, confirmed, and left to let them do their thing.\\nWhen I came to pick up later that afternoon, the ring was too small. It's very important to note that Shalimar sized the ring perfectly, but that I made a mistake and should've gone up a half-size.\\nThe Shalimar group were completely understanding and accommodating, even resizing my ring back up and getting it back to me within an hour at no charge! Even though it was my mistake!\\nThe associates' attitudes in dealing with what was a pretty embarrassing situation instantly earned my satisfaction and loyalty as a customer. Very grateful for such a wonderful experience.|2     |1WHRWwQmZOZDAhp2Qyny4g|\n",
      "|BVndHaLihEYbr76Z0CMEGw|0   |2014-10-11 16:22:06|0    |OAhBYw8IQ6wlfw1owXWRWw|5.0  |Great place for breakfast! I had the waffle, which was fluffy and perfect, and home fries which were nice and smashed and crunchy. Friendly waitstaff. Will definitely be back!                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |0     |1C2lxzUo1Hyye4RFIXly3g|\n",
      "|CLEWowfkj-wKYJlQDqT1aw|1   |2016-03-07 00:02:18|0    |u2vzZaOqJ2feRshaaF1doQ|5.0  |I go to blow bar to get my brows done by natalie (brow specialist) which i highly recommend she is great does a great job on my eyebrows! But then i got a blow by victoria!! Wow i was impress i have thin, straight, dead hair and she left me with the biggest volume ive ever had!!! Tried another girl but didnt like it as much so victoria will be my girl for ever; very beautiful clean place!!!                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |2     |NDZvyYHTUWWu-kqgQzzDGQ|\n",
      "|pR8u8hXf1vvzoAGOoKHQqQ|0   |2016-08-25 17:17:46|0    |EZarjNNbO_2yH1Xbizog9g|5.0  |First time here and they did a great job, very satisfied with the car wash  and the service I received. Thank you                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      |1     |R_W9WlKiA56VzVbRzTULQQ|\n",
      "|AgbRp5NLsP1-J1fdg6Hdcw|0   |2017-05-13 17:15:09|0    |5GGfWhVGmubQLmbVbye8vw|5.0  |Awesome little shop.  The owner really knows his stuff and you can tell he loves his work.  They have tires and other parts you won't find anywhere else.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |0     |B6G4f3UX1Z5_CpyV1xXm8Q|\n",
      "+----------------------+----+-------------------+-----+----------------------+-----+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------+----------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df3.na.drop('any').show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "59b95f29",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "986088"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3.count()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78f1d1cf",
   "metadata": {},
   "source": [
    "Repeat the same steps for the next table (checkin.json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "077f58bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[summary: string, business_id: string, date: string]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkin_path = './yelp_dataset/raw_data/yelp_academic_dataset_checkin.json'\n",
    "checkin_df = spark.read.json(checkin_path)\n",
    "checkin_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8142bbb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "131930"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkin_df.createOrReplaceTempView(\"checkin\")\n",
    "\n",
    "c_sqlDF = spark.sql('select * from checkin')\n",
    "c_sqlDF.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6653d3c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df4 = c_sqlDF.join(sqlDF,'business_id','leftsemi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a5bdf458",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18473"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df4.count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1c926929",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[summary: string, business_id: string, date: string]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df4.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa2c692f",
   "metadata": {},
   "source": [
    "### Aggregation playground \n",
    "\n",
    "From this point ownward things get messy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "bcbceec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df5 = df4.select(\"*\", explode(split('date', ',')).alias(\"exploded\")).groupBy(\"business_id\").agg(count(\"exploded\").alias(\"no_checkins\")).sort(\"no_checkins\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e079b858",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 60:>                                                         (0 + 8) / 8]\r",
      "\r",
      "[Stage 60:=======>                                                  (1 + 7) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------+\n",
      "|         business_id|no_checkins|\n",
      "+--------------------+-----------+\n",
      "|c_4c5rJECZSfNgFj7...|      37518|\n",
      "|QHWYlmVbLC3K6eglW...|       6820|\n",
      "|SwBhaxfQPbyhsi0QH...|       5394|\n",
      "|-K0LoSCfh8i5U_y53...|       4955|\n",
      "|3YqUe2FTCQr0pPVK8...|       4476|\n",
      "|AFYI0sfZ6WdVELjjE...|       4279|\n",
      "|fCDMLD21ypv1XZ_Ey...|       4202|\n",
      "|sihT-_DtwOdnDDDJb...|       4116|\n",
      "|LdECsE8lJS7v5GTFT...|       3936|\n",
      "|k-tHn9uxgWdq1ROHq...|       3862|\n",
      "+--------------------+-----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df5.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7a4c05de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[business_id: string, date: string]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_sqlDF.join(sqlDF,'business_id','leftsemi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d3fdf32b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = sqlDF.join(df5, sqlDF.business_id == df4.business_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "61e75cb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------------------+----------------+--------------------+-------+-------------+--------------+--------------------+-----------+------------+-----+-----+--------------------+-----------+\n",
      "|             address|          attributes|         business_id|          categories|            city|               hours|is_open|     latitude|     longitude|                name|postal_code|review_count|stars|state|         business_id|no_checkins|\n",
      "+--------------------+--------------------+--------------------+--------------------+----------------+--------------------+-------+-------------+--------------+--------------------+-----------+------------+-----+-----+--------------------+-----------+\n",
      "| 8150 Bryan Dairy Rd|{null, null, null...|vxrGNnuEef7YCfB7m...|Coffee & Tea, Don...|   Pinellas Park|{5:0-22:0, 5:0-22...|      1|   27.8718285|   -82.7502853|             Dunkin'|      33777|           8|  2.0|   FL|vxrGNnuEef7YCfB7m...|          2|\n",
      "|1101 Country Club...|{null, null, null...|sk2lZI4zmuGAccd3D...|Active Life, Park...|Saint Petersburg|{9:0-18:0, null, ...|      1|27.7251226771|-82.6500804812|Boyd Hill Nature ...|      33705|          33|  5.0|   FL|sk2lZI4zmuGAccd3D...|        115|\n",
      "|  10126 Woodberry Rd|{null, null, null...|3qbFLioR0HxfMs5-s...|Home Services, Ac...|           Tampa|{8:30-17:0, 8:30-...|      1|   27.9526465|   -82.3324836| Sharper Image Pools|      33619|           8|  3.5|   FL|3qbFLioR0HxfMs5-s...|          2|\n",
      "|1155 S Dale Mabry...|{null, null, null...|wS-SWAa_yaJAw6fJm...|Watch Repair, Loc...|           Tampa|{10:0-16:0, 0:0-0...|      1|    27.932634|    -82.506326|  Swiss Watch Center|      33629|          14|  3.5|   FL|wS-SWAa_yaJAw6fJm...|          4|\n",
      "|2310 N Dale Mabry...|{null, null, u'fu...|dXk1KSuQ3Bz1EP0Cz...|Lounges, Adult En...|           Tampa|{18:0-3:0, 18:0-3...|      1|   27.9620343|   -82.5060203|Scores Gentlemens...|      33607|          20|  2.5|   FL|dXk1KSuQ3Bz1EP0Cz...|        219|\n",
      "+--------------------+--------------------+--------------------+--------------------+----------------+--------------------+-------+-------------+--------------+--------------------+-----------+------------+-----+-----+--------------------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 65:=======>                                                  (1 + 7) / 8]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "test_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b023c964",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = df3.withColumn('week', weekofyear('date'))\n",
    "df3 = df3.withColumn('year', year('date'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4f33f38b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----+-------------------+-----+--------------------+-----+--------------------+------+--------------------+----+----+\n",
      "|         business_id|cool|               date|funny|           review_id|stars|                text|useful|             user_id|week|year|\n",
      "+--------------------+----+-------------------+-----+--------------------+-----+--------------------+------+--------------------+----+----+\n",
      "|uMvVYRgGNXf5boolA...|   0|2015-06-21 14:48:06|    0|rGQRf8UafX7OTlMNN...|  5.0|My experience wit...|     2|1WHRWwQmZOZDAhp2Q...|  25|2015|\n",
      "|BVndHaLihEYbr76Z0...|   0|2014-10-11 16:22:06|    0|OAhBYw8IQ6wlfw1ow...|  5.0|Great place for b...|     0|1C2lxzUo1Hyye4RFI...|  41|2014|\n",
      "|CLEWowfkj-wKYJlQD...|   1|2016-03-07 00:02:18|    0|u2vzZaOqJ2feRshaa...|  5.0|I go to blow bar ...|     2|NDZvyYHTUWWu-kqgQ...|  10|2016|\n",
      "|pR8u8hXf1vvzoAGOo...|   0|2016-08-25 17:17:46|    0|EZarjNNbO_2yH1Xbi...|  5.0|First time here a...|     1|R_W9WlKiA56VzVbRz...|  34|2016|\n",
      "|AgbRp5NLsP1-J1fdg...|   0|2017-05-13 17:15:09|    0|5GGfWhVGmubQLmbVb...|  5.0|Awesome little sh...|     0|B6G4f3UX1Z5_CpyV1...|  19|2017|\n",
      "+--------------------+----+-------------------+-----+--------------------+-----+--------------------+------+--------------------+----+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df3.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9aa18a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_data = df3.select(\"business_id\", \"stars\", \"week\", \"year\")\\\n",
    "                .groupBy(\"business_id\", \"week\", \"year\")\\\n",
    "                .agg(avg(\"stars\").alias(\"avg_stars\"))\\\n",
    "                .sort(\"avg_stars\", ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "addca56e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 73:==================================================>     (36 + 4) / 40]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----+----+---------+\n",
      "|         business_id|week|year|avg_stars|\n",
      "+--------------------+----+----+---------+\n",
      "|Xbu-HcPKCyLqyZCIL...|  45|2012|      5.0|\n",
      "|z55MjytBCcIza_LFT...|  34|2013|      5.0|\n",
      "|hGiiXUgQrl1oqY1Be...|  26|2015|      5.0|\n",
      "|3OGzmGqWwsyGLkhnx...|   6|2017|      5.0|\n",
      "|SwBhaxfQPbyhsi0QH...|   5|2012|      5.0|\n",
      "|QcuTCSfnI6WqKpuDL...|  40|2013|      5.0|\n",
      "|JQz0_R70G3bjQ5dRq...|  18|2017|      5.0|\n",
      "|ixPTo6Hum7nNZ7A4V...|  46|2014|      5.0|\n",
      "|kKPJLiHIr9Gd9sYs3...|  45|2015|      5.0|\n",
      "|quv6eMqyJ1NIkjPgI...|   4|2016|      5.0|\n",
      "+--------------------+----+----+---------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 75:==============>                                           (2 + 6) / 8]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "grouped_data.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b23339d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_data_avg = grouped_data.select(\"*\")\\\n",
    "    .groupBy(\"business_id\", \"year\")\\\n",
    "    .agg(avg(\"avg_stars\").alias(\"avg_stars_y\"))\\\n",
    "    .sort(\"avg_stars_y\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "07301cc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----+-----------+\n",
      "|         business_id|year|avg_stars_y|\n",
      "+--------------------+----+-----------+\n",
      "|CGZM3xRSaiYWPmZTt...|2019|        5.0|\n",
      "|FJue-hS7-ZYLv_tnI...|2021|        5.0|\n",
      "|mhPaO2zdKoI7lKRAi...|2021|        5.0|\n",
      "|pyM9OTnfDoM2naIYv...|2019|        5.0|\n",
      "|ABeyWKAGm3ISQ0Tq-...|2020|        5.0|\n",
      "|-ch3ET0QStiKPWo1V...|2015|        5.0|\n",
      "|gd22TmYLmbrgW6XEk...|2014|        5.0|\n",
      "|ymi8Cb93TUSN6wwlv...|2008|        5.0|\n",
      "|68vO3osEXU00U5uLw...|2021|        5.0|\n",
      "|hGn7G9rWHW9E2diFV...|2016|        5.0|\n",
      "|h1x65PeAa_rEfffzE...|2015|        5.0|\n",
      "|pyrM6aaTOgB7iDLr5...|2019|        5.0|\n",
      "|yuCHAXdpgbQdvDbzT...|2020|        5.0|\n",
      "|MSVowoBcMisrot9FP...|2021|        5.0|\n",
      "|e1rNGKIwNi528_J7Q...|2021|        5.0|\n",
      "|DXoWKZEVA_PW1o_cd...|2015|        5.0|\n",
      "|G0qAfGpIHU0gDJl-t...|2018|        5.0|\n",
      "|04zP1Y6kdBNBXUJ3m...|2018|        5.0|\n",
      "|xMhGSZMB85vnk3OVP...|2017|        5.0|\n",
      "|GtVgio1t55VYUm8JW...|2020|        5.0|\n",
      "+--------------------+----+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "grouped_data_avg.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c77728f4",
   "metadata": {},
   "source": [
    "### Aggregations start to take shape \n",
    "\n",
    "At this point I has an idea on how my python code would look like to I started to play around with my aggregations both in pycharm and jupyter notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d6b4fb1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "import logging\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import split, \\\n",
    "    explode, weekofyear, year, avg, when, count\n",
    "\n",
    "LOG_TAG = 'YelpDatasetAggregator'\n",
    "\n",
    "\n",
    "class YelpDatasetAggregator:\n",
    "\n",
    "    def __init__(self):\n",
    "        self._app_name = 'yelp_aggregator'\n",
    "        self._logger = logging.getLogger(LOG_TAG)\n",
    "        logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "        self._spark_session = False\n",
    "        try:\n",
    "            self._spark_session = SparkSession.builder \\\n",
    "                .appName(self._app_name) \\\n",
    "                .getOrCreate()\n",
    "            logging.info(f'Spark session for {self._app_name} '\n",
    "                         f'was successfully created')\n",
    "        except:\n",
    "            logging.info(f'Spark session for {self._app_name} '\n",
    "                         f'was unsuccessful')\n",
    "\n",
    "    def read_json_data(self,\n",
    "                       input_data,\n",
    "                       table_name='temp'):\n",
    "        \"\"\"\n",
    "        Reads data from json\n",
    "\n",
    "        Args:\n",
    "            input_data (string): Full path to json file with input data\n",
    "            table_name (string): Name of the temporary table\n",
    "\n",
    "        Returns:\n",
    "            (pyspark.sql.dataframe.DataFrame): Resulted cleaned up table\n",
    "        \"\"\"\n",
    "        df = ''\n",
    "\n",
    "        if os.path.exists(input_data):\n",
    "\n",
    "            self._logger.info(f'Reading {input_data}')\n",
    "            df = self._spark_session.read.json(input_data)\n",
    "            df.createOrReplaceTempView(table_name)\n",
    "\n",
    "        else:\n",
    "            self._logger.info(f'T input data path does not exist {input_data}')\n",
    "\n",
    "        return df\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5293270d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/08 13:33:19 WARN Utils: Your hostname, Nikolettas-MacBook-Pro.local resolves to a loopback address: 127.0.0.1; using 192.168.0.3 instead (on interface en0)\n",
      "22/11/08 13:33:19 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/08 13:33:19 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Spark session for yelp_aggregator was successfully created\n",
      "INFO:YelpDatasetAggregator:Reading ./yelp_dataset/clean_data/yelp_academic_dataset_business.json\n",
      "INFO:YelpDatasetAggregator:Reading ./yelp_dataset/clean_data/yelp_academic_dataset_review.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/08 13:33:23 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 2:=============>                                             (2 + 7) / 9]\r",
      "\r",
      "                                                                                \r",
      "INFO:YelpDatasetAggregator:Reading ./yelp_dataset/clean_data/yelp_academic_dataset_checkin.json\n"
     ]
    }
   ],
   "source": [
    "input_data = './yelp_dataset/clean_data'\n",
    "yelp_aggregator = YelpDatasetAggregator()\n",
    "\n",
    "businesses_df = yelp_aggregator.read_json_data(\n",
    "        os.path.join(input_data, 'yelp_academic_dataset_business.json')\n",
    "    )\n",
    "\n",
    "reviews_df = yelp_aggregator.read_json_data(\n",
    "        os.path.join(input_data, 'yelp_academic_dataset_review.json')\n",
    "    )\n",
    "\n",
    "checkin_df = yelp_aggregator.read_json_data(\n",
    "        os.path.join(input_data, 'yelp_academic_dataset_checkin.json')\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "62a1000d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----+----+----------------+\n",
      "|         business_id|week|year|avg_stars_weekly|\n",
      "+--------------------+----+----+----------------+\n",
      "|TzaHqnoOUlSDPF2wL...|  34|2018|             1.0|\n",
      "|eNM4YpOYxGqiQFn_p...|  24|2016|             1.0|\n",
      "|rxGXX-5oVduCREDI6...|  23|2021|             1.0|\n",
      "|btCV4udJp4Hzv7GYv...|  31|2021|             1.0|\n",
      "|VFNqQHHzvQ71k11Wu...|  42|2021|             1.0|\n",
      "|kBhu25HU_hfsmhUAA...|  33|2016|             1.0|\n",
      "|AUOk3xhNTyXShq3Mk...|  52|2021|             1.0|\n",
      "|f-7gzwLtoqis8Stcr...|  17|2021|             1.0|\n",
      "|w9qOUs1Nkyyiy0gIy...|   9|2019|             1.0|\n",
      "|qI2NdIdo3YK5ezlUB...|  16|2020|             1.0|\n",
      "+--------------------+----+----+----------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 11:>                                                         (0 + 8) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----+-----------------------+\n",
      "|         business_id|year|avg_stars_weekly_yearly|\n",
      "+--------------------+----+-----------------------+\n",
      "|ksma5FPvWkaZvcwHz...|2017|                    1.0|\n",
      "|pXcCvfcVWLRTFiicH...|2022|                    1.0|\n",
      "|GEdrmeb5ubO2Gq5Mg...|2016|                    1.0|\n",
      "|sG13h54FVCanFhwgw...|2021|                    1.0|\n",
      "|f_xxwfH2g_WkOdtDq...|2019|                    1.0|\n",
      "|RgiKENuItZe1LaeEz...|2020|                    1.0|\n",
      "|R8tmRzfmYX9Dx5HaL...|2015|                    1.0|\n",
      "|dogcOCiqIr3Q09VW_...|2020|                    1.0|\n",
      "|bbDaL2a1jYrfNK6p9...|2018|                    1.0|\n",
      "|IWRg2QAqkMfyb3M07...|2013|                    1.0|\n",
      "+--------------------+----+-----------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Calculate weekly average stars per business\n",
    "reviews_df = reviews_df.withColumn('week', weekofyear('date'))\n",
    "reviews_df = reviews_df.withColumn('year', year('date'))\n",
    "\n",
    "grouped_weekly = reviews_df\\\n",
    "    .select(\"*\")\\\n",
    "    .groupBy(\"business_id\", \"week\", \"year\")\\\n",
    "    .agg(avg(\"stars\").alias(\"avg_stars_weekly\"))\\\n",
    "    .sort(\"avg_stars_weekly\", ascending=True)\n",
    "\n",
    "grouped_weekly.show(10)\n",
    "\n",
    "grouped_weekly_yearly = grouped_weekly\\\n",
    "    .select(\"*\")\\\n",
    "    .groupBy(\"business_id\", \"year\")\\\n",
    "    .agg(avg(\"avg_stars_weekly\").alias(\"avg_stars_weekly_yearly\"))\\\n",
    "    .sort(\"avg_stars_weekly_yearly\", ascending=True)\n",
    "\n",
    "grouped_weekly_yearly.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "68c6c71a",
   "metadata": {},
   "outputs": [],
   "source": [
    "businesses_reviews_weekly = businesses_df.\\\n",
    "    join(grouped_weekly, 'business_id')\n",
    "businesses_reviews_weekly_checkins = businesses_reviews_weekly.\\\n",
    "    join(checkin_df, 'business_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2a0cc811",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[business_id: string, address: string, attributes: struct<AcceptsInsurance:string,AgesAllowed:string,Alcohol:string,Ambience:string,BYOB:string,BYOBCorkage:string,BestNights:string,BikeParking:string,BusinessAcceptsBitcoin:string,BusinessAcceptsCreditCards:string,BusinessParking:string,ByAppointmentOnly:string,Caters:string,CoatCheck:string,Corkage:string,DietaryRestrictions:string,DogsAllowed:string,DriveThru:string,GoodForDancing:string,GoodForKids:string,GoodForMeal:string,HairSpecializesIn:string,HappyHour:string,HasTV:string,Music:string,NoiseLevel:string,Open24Hours:string,OutdoorSeating:string,RestaurantsAttire:string,RestaurantsCounterService:string,RestaurantsDelivery:string,RestaurantsGoodForGroups:string,RestaurantsPriceRange2:string,RestaurantsReservations:string,RestaurantsTableService:string,RestaurantsTakeOut:string,Smoking:string,WheelchairAccessible:string,WiFi:string>, categories: string, city: string, hours: struct<Friday:string,Monday:string,Saturday:string,Sunday:string,Thursday:string,Tuesday:string,Wednesday:string>, is_open: bigint, latitude: double, longitude: double, name: string, postal_code: string, review_count: bigint, stars: double, state: string, week: int, year: int, avg_stars_weekly: double, date: string]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "businesses_reviews_weekly_checkins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fa7661cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[business_id: string, address: string, attributes: struct<AcceptsInsurance:string,AgesAllowed:string,Alcohol:string,Ambience:string,BYOB:string,BYOBCorkage:string,BestNights:string,BikeParking:string,BusinessAcceptsBitcoin:string,BusinessAcceptsCreditCards:string,BusinessParking:string,ByAppointmentOnly:string,Caters:string,CoatCheck:string,Corkage:string,DietaryRestrictions:string,DogsAllowed:string,DriveThru:string,GoodForDancing:string,GoodForKids:string,GoodForMeal:string,HairSpecializesIn:string,HappyHour:string,HasTV:string,Music:string,NoiseLevel:string,Open24Hours:string,OutdoorSeating:string,RestaurantsAttire:string,RestaurantsCounterService:string,RestaurantsDelivery:string,RestaurantsGoodForGroups:string,RestaurantsPriceRange2:string,RestaurantsReservations:string,RestaurantsTableService:string,RestaurantsTakeOut:string,Smoking:string,WheelchairAccessible:string,WiFi:string>, categories: string, city: string, hours: struct<Friday:string,Monday:string,Saturday:string,Sunday:string,Thursday:string,Tuesday:string,Wednesday:string>, is_open: bigint, latitude: double, longitude: double, name: string, postal_code: string, review_count: bigint, stars: double, state: string, week: int, year: int, avg_stars_weekly: double, date: string]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "businesses_reviews_weekly_checkins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "03fd9ce0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 27:==================================================>       (7 + 1) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+----------------+-----+-----------+\n",
      "|                name|          categories|avg_stars_weekly|stars|no_checkins|\n",
      "+--------------------+--------------------+----------------+-----+-----------+\n",
      "|Tucson Internatio...|Professional Serv...|             4.0|  4.0|    1049256|\n",
      "|Tucson Internatio...|Professional Serv...|             5.0|  4.0|     928188|\n",
      "|HUB Restaurant & ...|Ice Cream & Froze...|             4.0|  4.0|     480109|\n",
      "|HUB Restaurant & ...|Ice Cream & Froze...|             5.0|  4.0|     385882|\n",
      "|            Cup Cafe|Breakfast & Brunc...|             5.0|  4.0|     371966|\n",
      "|Tucson Internatio...|Professional Serv...|             3.0|  4.0|     343026|\n",
      "|      Barrio Brewing|Nightlife, Sports...|             4.0|  4.0|     338940|\n",
      "|   Bobo's Restaurant|Burgers, Sandwich...|             5.0|  4.5|     337120|\n",
      "|     El Guero Canelo|Mexican, Restaurants|             5.0|  4.0|     309583|\n",
      "|         Miss Saigon|Vegetarian, Vietn...|             5.0|  4.0|     299222|\n",
      "+--------------------+--------------------+----------------+-----+-----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "businesses_reviews_weekly_checkins\\\n",
    "        .select(\"*\", explode(split('date', ',')).alias(\"exploded\"))\\\n",
    "        .groupBy(\"name\", \"categories\", \"avg_stars_weekly\", \"stars\") \\\n",
    "        .agg(count(\"exploded\").alias(\"no_checkins\"))\\\n",
    "        .sort(\"no_checkins\", ascending=False)\\\n",
    "        .show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2f96de8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting date in checkins to count the entries per week and year\n",
    "checkin_df = checkin_df.withColumn('exploded_date', explode(split('date', ',')))\n",
    "checkin_df = checkin_df.withColumn('week', weekofyear('exploded_date'))\n",
    "checkin_df = checkin_df.withColumn('year', year('exploded_date'))\n",
    "\n",
    "checkin_df = checkin_df.drop('exploded_date')\n",
    "\n",
    "# Combining businesses, weekly stars, overall stars and checkins\n",
    "businesses_reviews_weekly = businesses_df.\\\n",
    "    join(grouped_weekly, 'business_id')\n",
    "businesses_reviews_weekly_checkins = businesses_reviews_weekly.\\\n",
    "    join(checkin_df, # 'business_id')\n",
    "         (businesses_reviews_weekly.business_id == checkin_df.business_id) & \\\n",
    "         (businesses_reviews_weekly.week == checkin_df.week) & \\\n",
    "         (businesses_reviews_weekly.year == checkin_df.year))\n",
    "\n",
    "# businesses_reviews_weekly_checkins\\\n",
    "#     .select(\"*\", explode(split('date', ',')).alias(\"exploded\"))\\\n",
    "#     .groupBy(\"name\", \"categories\", \"week\", \"year\", \"avg_stars_weekly\", \"stars\") \\\n",
    "#     .agg(count(\"exploded\").alias(\"no_checkins\"))\\\n",
    "#     .sort(\"no_checkins\", ascending=False)\\\n",
    "#     .show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "0befa813",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 128:>                                                        (0 + 8) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1115.360s][warning][gc,alloc] Executor task launch worker for task 2.0 in stage 128.0 (TID 837): Retried waiting for GCLocker too often allocating 504 words\n",
      "[1116.554s][warning][gc,alloc] Executor task launch worker for task 6.0 in stage 128.0 (TID 841): Retried waiting for GCLocker too often allocating 1559 words\n",
      "22/11/08 13:32:11 ERROR Executor: Exception in task 6.0 in stage 128.0 (TID 841)\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "\tat org.apache.spark.sql.catalyst.expressions.UnsafeRow.copy(UnsafeRow.java:492)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.generate_doConsume_1$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.generate_doConsume_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:364)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$Lambda$4155/0x0000000801734040.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)\n",
      "\tat org.apache.spark.rdd.RDD$$Lambda$2534/0x00000008011acc40.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$2423/0x00000008010e7440.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "22/11/08 13:32:11 ERROR Executor: Exception in task 2.0 in stage 128.0 (TID 837)\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "\tat org.apache.spark.sql.catalyst.expressions.UnsafeRow.copy(UnsafeRow.java:492)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.generate_doConsume_1$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.generate_doConsume_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:364)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$Lambda$4155/0x0000000801734040.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)\n",
      "\tat org.apache.spark.rdd.RDD$$Lambda$2534/0x00000008011acc40.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$2423/0x00000008010e7440.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "22/11/08 13:32:11 ERROR Executor: Exception in task 3.0 in stage 128.0 (TID 838)\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "22/11/08 13:32:11 ERROR SparkUncaughtExceptionHandler: Uncaught exception in thread Thread[Executor task launch worker for task 2.0 in stage 128.0 (TID 837),5,main]\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "\tat org.apache.spark.sql.catalyst.expressions.UnsafeRow.copy(UnsafeRow.java:492)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.generate_doConsume_1$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.generate_doConsume_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:364)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$Lambda$4155/0x0000000801734040.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)\n",
      "\tat org.apache.spark.rdd.RDD$$Lambda$2534/0x00000008011acc40.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$2423/0x00000008010e7440.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "22/11/08 13:32:11 ERROR SparkUncaughtExceptionHandler: Uncaught exception in thread Thread[Executor task launch worker for task 6.0 in stage 128.0 (TID 841),5,main]\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "\tat org.apache.spark.sql.catalyst.expressions.UnsafeRow.copy(UnsafeRow.java:492)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.generate_doConsume_1$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.generate_doConsume_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:364)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$Lambda$4155/0x0000000801734040.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)\n",
      "\tat org.apache.spark.rdd.RDD$$Lambda$2534/0x00000008011acc40.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$2423/0x00000008010e7440.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "22/11/08 13:32:11 ERROR SparkUncaughtExceptionHandler: [Container in shutdown] Uncaught exception in thread Thread[Executor task launch worker for task 3.0 in stage 128.0 (TID 838),5,main]\n",
      "java.lang.OutOfMemoryError: Java heap space\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/08 13:32:12 ERROR Executor: Exception in task 0.0 in stage 128.0 (TID 835)\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "22/11/08 13:32:12 ERROR SparkUncaughtExceptionHandler: [Container in shutdown] Uncaught exception in thread Thread[Executor task launch worker for task 0.0 in stage 128.0 (TID 835),5,main]\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "22/11/08 13:32:12 WARN TaskSetManager: Lost task 2.0 in stage 128.0 (TID 837) (192.168.0.3 executor driver): UnknownReason\n",
      "22/11/08 13:32:12 ERROR TaskSetManager: Task 2 in stage 128.0 failed 1 times; aborting job\n",
      "22/11/08 13:32:12 WARN TaskSetManager: Lost task 3.0 in stage 128.0 (TID 838) (192.168.0.3 executor driver): java.lang.OutOfMemoryError: Java heap space\n",
      "\n",
      "22/11/08 13:32:12 ERROR Utils: Uncaught exception in thread task-result-getter-3\n",
      "java.lang.OutOfMemoryError: Java heap space\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread \"task-result-getter-3\" java.lang.OutOfMemoryError: Java heap space\n",
      "Exception in thread \"Thread-2\" java.lang.OutOfMemoryError: Java heap space) / 8]\n",
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/nxirakia/miniforge3/envs/Yelp_Data_Engineering/lib/python3.10/site-packages/pyspark/sql/utils.py\", line 190, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/Users/nxirakia/miniforge3/envs/Yelp_Data_Engineering/lib/python3.10/site-packages/py4j/protocol.py\", line 326, in get_return_value\n",
      "    raise Py4JJavaError(\n",
      "py4j.protocol.Py4JJavaError: An error occurred while calling o776.showString.\n",
      ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 2 in stage 128.0 failed 1 times, most recent failure: Lost task 2.0 in stage 128.0 (TID 837) (192.168.0.3 executor driver): UnknownReason\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2228)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2249)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2268)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2293)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1021)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\n",
      "\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1020)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeCollectIterator(SparkPlan.scala:431)\n",
      "\tat org.apache.spark.sql.execution.exchange.BroadcastExchangeExec.$anonfun$relationFuture$1(BroadcastExchangeExec.scala:137)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withThreadLocalCaptured$1(SQLExecution.scala:191)\n",
      "\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/nxirakia/miniforge3/envs/Yelp_Data_Engineering/lib/python3.10/site-packages/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "  File \"/Users/nxirakia/miniforge3/envs/Yelp_Data_Engineering/lib/python3.10/socket.py\", line 705, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "ConnectionResetError: [Errno 54] Connection reset by peer\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/nxirakia/miniforge3/envs/Yelp_Data_Engineering/lib/python3.10/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/Users/nxirakia/miniforge3/envs/Yelp_Data_Engineering/lib/python3.10/site-packages/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/08 13:32:15 WARN SparkContext: Ignoring Exception while stopping SparkContext from shutdown hook\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "22/11/08 13:32:15 WARN TaskSetManager: Lost task 7.0 in stage 128.0 (TID 842) (192.168.0.3 executor driver): TaskKilled (Stage cancelled)\n",
      "22/11/08 13:32:15 ERROR Executor: Exception in task 1.0 in stage 128.0 (TID 836)\n",
      "java.lang.OutOfMemoryError: Java heap space: failed reallocation of scalar replaced objects\n",
      "22/11/08 13:32:15 ERROR SparkUncaughtExceptionHandler: [Container in shutdown] Uncaught exception in thread Thread[Executor task launch worker for task 1.0 in stage 128.0 (TID 836),5,main]\n",
      "java.lang.OutOfMemoryError: Java heap space: failed reallocation of scalar replaced objects\n",
      "22/11/08 13:32:15 ERROR Executor: Exception in task 4.0 in stage 128.0 (TID 839)\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "22/11/08 13:32:15 ERROR SparkUncaughtExceptionHandler: [Container in shutdown] Uncaught exception in thread Thread[Executor task launch worker for task 4.0 in stage 128.0 (TID 839),5,main]\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "22/11/08 13:32:15 ERROR Executor: Exception in task 5.0 in stage 128.0 (TID 840)\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "22/11/08 13:32:15 ERROR SparkUncaughtExceptionHandler: [Container in shutdown] Uncaught exception in thread Thread[Executor task launch worker for task 5.0 in stage 128.0 (TID 840),5,main]\n",
      "java.lang.OutOfMemoryError: Java heap space\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Exception while sending command.                         (0 + 4) / 8]\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/nxirakia/miniforge3/envs/Yelp_Data_Engineering/lib/python3.10/site-packages/pyspark/sql/utils.py\", line 190, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/Users/nxirakia/miniforge3/envs/Yelp_Data_Engineering/lib/python3.10/site-packages/py4j/protocol.py\", line 326, in get_return_value\n",
      "    raise Py4JJavaError(\n",
      "py4j.protocol.Py4JJavaError: <unprintable Py4JJavaError object>\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/nxirakia/miniforge3/envs/Yelp_Data_Engineering/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3433, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/var/folders/nn/jyjrmyb96217hmy9vnyx78nr0000gn/T/ipykernel_178/3818012214.py\", line 1, in <module>\n",
      "    businesses_reviews_weekly_checkins.show(5)\n",
      "  File \"/Users/nxirakia/miniforge3/envs/Yelp_Data_Engineering/lib/python3.10/site-packages/pyspark/sql/dataframe.py\", line 606, in show\n",
      "    print(self._jdf.showString(n, 20, vertical))\n",
      "  File \"/Users/nxirakia/miniforge3/envs/Yelp_Data_Engineering/lib/python3.10/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/nxirakia/miniforge3/envs/Yelp_Data_Engineering/lib/python3.10/site-packages/pyspark/sql/utils.py\", line 192, in deco\n",
      "    converted = convert_exception(e.java_exception)\n",
      "  File \"/Users/nxirakia/miniforge3/envs/Yelp_Data_Engineering/lib/python3.10/site-packages/pyspark/sql/utils.py\", line 156, in convert_exception\n",
      "    elif is_instance_of(gw, e, \"org.apache.spark.sql.AnalysisException\"):\n",
      "  File \"/Users/nxirakia/miniforge3/envs/Yelp_Data_Engineering/lib/python3.10/site-packages/py4j/java_gateway.py\", line 464, in is_instance_of\n",
      "    return gateway.jvm.py4j.reflection.TypeUtil.isInstanceOf(\n",
      "  File \"/Users/nxirakia/miniforge3/envs/Yelp_Data_Engineering/lib/python3.10/site-packages/py4j/java_gateway.py\", line 1547, in __getattr__\n",
      "    raise Py4JError(\n",
      "py4j.protocol.Py4JError: py4j.reflection.TypeUtil.isInstanceOf does not exist in the JVM\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/nxirakia/miniforge3/envs/Yelp_Data_Engineering/lib/python3.10/site-packages/py4j/clientserver.py\", line 516, in send_command\n",
      "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
      "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/nxirakia/miniforge3/envs/Yelp_Data_Engineering/lib/python3.10/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/Users/nxirakia/miniforge3/envs/Yelp_Data_Engineering/lib/python3.10/site-packages/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n"
     ]
    },
    {
     "ename": "Py4JError",
     "evalue": "py4j.reflection.TypeUtil.isInstanceOf does not exist in the JVM",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[0;32m~/miniforge3/envs/Yelp_Data_Engineering/lib/python3.10/site-packages/pyspark/sql/utils.py:190\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    191\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/miniforge3/envs/Yelp_Data_Engineering/lib/python3.10/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31m<class 'str'>\u001b[0m: (<class 'py4j.protocol.Py4JError'>, Py4JError('An error occurred while calling None.None'))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mPy4JError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [62], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mbusinesses_reviews_weekly_checkins\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/Yelp_Data_Engineering/lib/python3.10/site-packages/pyspark/sql/dataframe.py:606\u001b[0m, in \u001b[0;36mDataFrame.show\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    603\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParameter \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvertical\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m must be a bool\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    605\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(truncate, \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m truncate:\n\u001b[0;32m--> 606\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshowString\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    607\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    608\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniforge3/envs/Yelp_Data_Engineering/lib/python3.10/site-packages/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m~/miniforge3/envs/Yelp_Data_Engineering/lib/python3.10/site-packages/pyspark/sql/utils.py:192\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[1;32m    191\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 192\u001b[0m     converted \u001b[38;5;241m=\u001b[39m \u001b[43mconvert_exception\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjava_exception\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    193\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    194\u001b[0m         \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    195\u001b[0m         \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[1;32m    196\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n",
      "File \u001b[0;32m~/miniforge3/envs/Yelp_Data_Engineering/lib/python3.10/site-packages/pyspark/sql/utils.py:156\u001b[0m, in \u001b[0;36mconvert_exception\u001b[0;34m(e)\u001b[0m\n\u001b[1;32m    154\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ParseException(origin\u001b[38;5;241m=\u001b[39me)\n\u001b[1;32m    155\u001b[0m \u001b[38;5;66;03m# Order matters. ParseException inherits AnalysisException.\u001b[39;00m\n\u001b[0;32m--> 156\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[43mis_instance_of\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43morg.apache.spark.sql.AnalysisException\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    157\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m AnalysisException(origin\u001b[38;5;241m=\u001b[39me)\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_instance_of(gw, e, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124morg.apache.spark.sql.streaming.StreamingQueryException\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/miniforge3/envs/Yelp_Data_Engineering/lib/python3.10/site-packages/py4j/java_gateway.py:464\u001b[0m, in \u001b[0;36mis_instance_of\u001b[0;34m(gateway, java_object, java_class)\u001b[0m\n\u001b[1;32m    460\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    461\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    462\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjava_class must be a string, a JavaClass, or a JavaObject\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 464\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgateway\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpy4j\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreflection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTypeUtil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43misInstanceOf\u001b[49m(\n\u001b[1;32m    465\u001b[0m     param, java_object)\n",
      "File \u001b[0;32m~/miniforge3/envs/Yelp_Data_Engineering/lib/python3.10/site-packages/py4j/java_gateway.py:1547\u001b[0m, in \u001b[0;36mJavaClass.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1544\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m get_return_value(\n\u001b[1;32m   1545\u001b[0m             answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fqn, name)\n\u001b[1;32m   1546\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1547\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m   1548\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m does not exist in the JVM\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fqn, name))\n",
      "\u001b[0;31mPy4JError\u001b[0m: py4j.reflection.TypeUtil.isInstanceOf does not exist in the JVM"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/nxirakia/miniforge3/envs/Yelp_Data_Engineering/lib/python3.10/site-packages/py4j/clientserver.py\", line 516, in send_command\n",
      "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
      "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/nxirakia/miniforge3/envs/Yelp_Data_Engineering/lib/python3.10/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/Users/nxirakia/miniforge3/envs/Yelp_Data_Engineering/lib/python3.10/site-packages/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n"
     ]
    }
   ],
   "source": [
    "businesses_reviews_weekly_checkins.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "215ceb86",
   "metadata": {},
   "source": [
    "Some nice errors that show that my querying skills are not optimal "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "373bd862",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+----+----+\n",
      "|         business_id|                date|week|year|\n",
      "+--------------------+--------------------+----+----+\n",
      "|c4P7jdIvKN-9Ol75p...|2011-10-25 00:22:...|  43|2011|\n",
      "|c4P7jdIvKN-9Ol75p...|2011-10-25 00:22:...|  46|2011|\n",
      "|c4P7jdIvKN-9Ol75p...|2011-10-25 00:22:...|  46|2011|\n",
      "|c4P7jdIvKN-9Ol75p...|2011-10-25 00:22:...|  11|2012|\n",
      "|c4P7jdIvKN-9Ol75p...|2011-10-25 00:22:...|  11|2012|\n",
      "|c4P7jdIvKN-9Ol75p...|2011-10-25 00:22:...|  25|2012|\n",
      "|c4P7jdIvKN-9Ol75p...|2011-10-25 00:22:...|  44|2012|\n",
      "|c4P7jdIvKN-9Ol75p...|2011-10-25 00:22:...|  45|2012|\n",
      "|c4P7jdIvKN-9Ol75p...|2011-10-25 00:22:...|  18|2013|\n",
      "|c4P7jdIvKN-9Ol75p...|2011-10-25 00:22:...|  26|2013|\n",
      "+--------------------+--------------------+----+----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "checkin_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "67f26c53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----+----+---------------+\n",
      "|         business_id|week|year|weekly_checkins|\n",
      "+--------------------+----+----+---------------+\n",
      "|-DxeodWsu1zuv_4ND...|   7|2016|             54|\n",
      "|t_fm_mMoVyArU9bzR...|  41|2013|             51|\n",
      "|p76UGLhyXLxUZij1M...|   2|2022|             49|\n",
      "|DNMdZVVfhus2H-dH0...|  41|2014|             49|\n",
      "|ZJXc_MYDaoNr2n4uR...|  34|2011|             46|\n",
      "|Aa0EldaVchavAAWL1...|  21|2014|             45|\n",
      "|Aa0EldaVchavAAWL1...|  45|2013|             44|\n",
      "|Aa0EldaVchavAAWL1...|  39|2013|             44|\n",
      "|Aa0EldaVchavAAWL1...|  42|2012|             43|\n",
      "|Aa0EldaVchavAAWL1...|  24|2012|             41|\n",
      "+--------------------+----+----+---------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "checkin_df_weekly = checkin_df\\\n",
    "    .select(\"*\").groupBy(\"business_id\", \"week\", \"year\")\\\n",
    "    .agg(count(\"week\").alias(\"weekly_checkins\"))\\\n",
    "    .sort(\"weekly_checkins\", ascending=False)\n",
    "\n",
    "checkin_df_weekly.show(10)\n",
    "\n",
    "# checkin_df_yearly = checkin_df\\\n",
    "#     .select(\"*\").groupBy(\"business_id\", \"year\")\\\n",
    "#     .agg(count(\"year\").alias(\"no_yearly_checkins\"))\\\n",
    "#     .sort(\"no_yearly_checkins\", ascending=False)\n",
    "# checkin_df_yearly.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c0ee0dd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
